{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# komet : Kronecker Optimized METhod for DTI prediction\n",
        "\n",
        "1. Downloading atils and the dataset (train/val/test) from Github \n",
        "2. Calculation of molecule features using a subsample of train molecules (MorganFP kernel approximated via Nystrom approximation)\n",
        "3. Calculation of protein features (SVD on LAkernel)\n",
        "4. Train/Testing with a chosen lambda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import pickle\n",
        "\n",
        "import time \n",
        "\n",
        "from sklearn.metrics import  average_precision_score,  roc_curve, confusion_matrix, precision_score, recall_score, auc\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device_cpu = torch.device(\"cpu\")\n",
        "device_cpu = device\n",
        "print( device )\n",
        "\n",
        "mytype = torch.float16 # to save memory (only on GPU)\n",
        "mytype = torch.float32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install rdkit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Download the data from a GitHub repo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Download utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget -q https://github.com/Guichaoua/komet/raw/main/komet/komet.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import komet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Download train/val/test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir data/\n",
        "!wget -q https://github.com/Guichaoua/komet/raw/main/data/LCIdb/Orphan/test.csv\n",
        "!mv test.csv data/\n",
        "!wget -q https://github.com/Guichaoua/komet/raw/main/data/LCIdb/Orphan/train.csv.zip\n",
        "!mv train.csv.zip data/\n",
        "!wget -q https://github.com/Guichaoua/komet/raw/main/data/LCIdb/Orphan/val.csv\n",
        "!mv val.csv data/\n",
        "!wget -q https://github.com/Guichaoua/komet/raw/main/data/LCIdb/Orphan/dict_ind2fasta_LCIdb.data\n",
        "!mv dict_ind2fasta_LCIdb.data data/\n",
        "!wget -q https://github.com/Guichaoua/komet/raw/main/data/LCIdb/Orphan/dict_ind2fasta_LCIdb_K_prot.data\n",
        "!mv dict_ind2fasta_LCIdb_K_prot.data data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_dir = \"data/\"\n",
        "\n",
        "# load data\n",
        "train = komet.load_df(\"train.csv.zip\",dataset_dir)\n",
        "val = komet.load_df(\"val.csv\",dataset_dir)\n",
        "test = komet.load_df(\"test.csv\",dataset_dir)\n",
        "\n",
        "# dataframe full has all smiles and fasta sequences\n",
        "full = pd.concat([train, val, test])\n",
        "print(\"full shape\",full.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Calculation of molecule features using a subsample of train molecules (molecule kernel approximated via Nystrom approximation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#### MOLECULE####\n",
        "# Index of the smiles in the dataset\n",
        "list_smiles = full[['SMILES']].drop_duplicates().values.flatten()\n",
        "nM = len(list_smiles)\n",
        "print(\"number of different smiles (mol):\",nM)\n",
        "dict_smiles2ind = {list_smiles[i]:i for i in range(nM)}\n",
        "\n",
        "# add indsmiles in train, val, test\n",
        "train['indsmiles'] = train['SMILES'].apply(lambda x:dict_smiles2ind[x] )\n",
        "val['indsmiles'] = val['SMILES'].apply(lambda x: dict_smiles2ind[x])\n",
        "test['indsmiles'] = test['SMILES'].apply(lambda x: dict_smiles2ind[x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# molecule kernel_first step : compute Morgan FP for each smiles of all the dataset\n",
        "MorganFP = komet.Morgan_FP(list_smiles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Choice of the parameters for the Nystrom approximation and the reduction dimension of the features \n",
        "mM = 3000 #all mol to compute the mol kernel for medium-scale database\n",
        "dM = 1000 #all dim for the mol features for medium-scale database\n",
        "\n",
        "# In case there are less molecules than the number of molecules to compute the Nystrom approximation\n",
        "mM = min(mM,nM) # number of molecule to compute nystrom\n",
        "dM = min(dM,nM) # final dimension of features for molecules\n",
        "print(\"mM\",mM,\"dM\",dM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# compute the Nystrom approximation of the mol kernel and the features of the Kronecker kernel (features normalized and calculated on all mol contained in the dataset (train/val/test))\n",
        "X_cn = komet.Nystrom_X_cn(mM,dM,nM,MorganFP)\n",
        "print(\"mol features shape\",X_cn.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3S4lGo5n77C"
      },
      "source": [
        "### 3. Calculation of protein features "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Index of the protein in the dataset\n",
        "fasta = full[['Target Sequence']].drop_duplicates().values.flatten() # fasta sequence on the dataset, in the same order as the dataset\n",
        "print(\"number of different Fasta (protein):\",len(fasta))\n",
        "# add ind_fasta dans train, val et test\n",
        "train['indfasta'] = train['Target Sequence'].apply(lambda x: np.where(fasta==x)[0][0])\n",
        "val['indfasta'] = val['Target Sequence'].apply(lambda x: np.where(fasta==x)[0][0])\n",
        "test['indfasta'] = test['Target Sequence'].apply(lambda x:  np.where(fasta==x)[0][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Protein kernel and dictionary of index\n",
        "dict_ind2fasta_LCIdb = pickle.load(open(dataset_dir + \"dict_ind2fasta_LCIdb.data\", 'rb'))\n",
        "dict_fasta2ind_LCIdb = {fasta:ind for ind,fasta in dict_ind2fasta_LCIdb.items()}\n",
        "with open(dataset_dir + \"dict_ind2fasta_LCIdb_K_prot.data\", 'rb') as f:\n",
        "    KP_LCIdb = pickle.load(f)\n",
        "KP_LCIdb.shape, type(KP_LCIdb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Protein kernel for the dataset\n",
        "I_fasta = [int(dict_fasta2ind_LCIdb[fasta[i]]) for i in range(len(fasta))] # index of fasta in the precomputed dict and protein kernel, in the same order as the dataset\n",
        "KP = KP_LCIdb[I_fasta,:][:,I_fasta]\n",
        "KP = torch.tensor(KP, dtype=mytype).to(device)\n",
        "print(\"kernel prot shape\",KP.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# computation of feature for protein (no nystrom, just SVD)\n",
        "rP = KP.shape[0]#min(KP.shape[0],500)\n",
        "U, Lambda, VT = torch.svd(KP)\n",
        "Y = U[:,:rP] @ torch.diag(torch.sqrt(Lambda[:rP]))\n",
        "\n",
        "# nomramlisation of the features\n",
        "Y_c = Y - Y.mean(axis = 0)\n",
        "Y_cn = Y_c / torch.norm(Y_c,dim = 1)[:,None]\n",
        "print(\"protein features shape\",Y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3XpBFstnvcX"
      },
      "source": [
        "### Load interactions index for train and test "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train\n",
        "I, J, y = komet.load_datas(train)\n",
        "n = len(I)\n",
        "print(\"len(train)\",n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test\n",
        "I_test, J_test, y_test = komet.load_datas(test)\n",
        "n_test = len(I_test)\n",
        "print(\"len(test)\",n_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training/Testing with a choosen lambda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#### TRAINING ####\n",
        "lamb = 1e-6\n",
        "# train the model\n",
        "w_bfgs,b_bfgs,history_lbfgs_SVM = komet.SVM_bfgs(X_cn,Y_cn,y,I,J,lamb,niter=50)\n",
        "# compute a probability using weights (Platt scaling)\n",
        "s,t,history_lbfgs_Platt = komet.compute_proba_Platt_Scalling(w_bfgs,X_cn,Y_cn,y,I,J,niter=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#### TRAIN ####\n",
        "# we compute a probability using weights (Platt scaling)\n",
        "m_train,y_pred_train, proba_pred_train = komet.compute_proba(w_bfgs,b_bfgs,s,t,X_cn,Y_cn,I,J)\n",
        "# we compute the results\n",
        "acc1_train,au_Roc_train,au_PR_train,thred_optim_train,acc_best_train,cm_train,FP_train = komet.results(y.cpu(),y_pred_train.cpu(),proba_pred_train.cpu())\n",
        "print(f\"roc AUC = {au_Roc_train:.4f}\")\n",
        "print(f\"AUPR = {au_PR_train:.4f}\")\n",
        "print(f\"accuracy (threshold 0.5)= {acc1_train:.4f}\")\n",
        "print(f\"best threshold = {thred_optim_train:.4f}\")\n",
        "print(f\"accuracy (best threshold)= {acc_best_train:.4f}\")\n",
        "print(f\"false positive (best threshold)= {FP_train:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#### TEST ####\n",
        "# we compute a probability using weights (Platt scaling)\n",
        "m,y_pred, proba_pred = komet.compute_proba(w_bfgs,b_bfgs,s,t,X_cn,Y_cn,I_test,J_test)\n",
        "# we compute the results\n",
        "acc1,au_Roc,au_PR,thred_optim,acc_best,cm,FP = komet.results(y_test.cpu(),y_pred.cpu(),proba_pred.cpu())\n",
        "print(f\"roc AUC = {au_Roc:.4f}\")\n",
        "print(f\"AUPR = {au_PR:.4f}\")\n",
        "print(f\"accuracy (threshold 0.5)= {acc1:.4f}\")\n",
        "print(f\"best threshold = {thred_optim:.4f}\")\n",
        "print(f\"accuracy (best threshold)= {acc_best:.4f}\")\n",
        "print(f\"false positive (best threshold)= {FP:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot confusion matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "labels = [-1., 1.]\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                              display_labels=labels)\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot distribution (density) of p when y_test=1\n",
        "plt.hist(proba_pred.cpu().numpy()[y_test.cpu().numpy()==1],bins=10,alpha=0.8,color='green',label='y_test=1');\n",
        "plt.hist(proba_pred.cpu().numpy()[y_test.cpu()==-1],bins=10,alpha=0.5,color='red',label='y_test=-1');\n",
        "plt.legend()\n",
        "plt.title('Distribution of predicted probability')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
