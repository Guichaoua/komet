{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# komet : Kronecker Optimized METhod for DTI prediction\n",
        "\n",
        "1. Downloading the dataset (train/val/test) : you can choose different databases \n",
        "2. Choosing parameters of approximation (Nystrom and features dimension)\n",
        "3. Calculating of molecule features using a subsample of train molecules (MorganFP kernel approximated via Nystrom approximation)\n",
        "4. Loading approximated protein features, using SVD of the Local Alignment kernel precalculated on 20605 human proteins\n",
        "5. Searching for the best lambda by choosing the best AUPR on the validation dataset\n",
        "6. Testing with best lambda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import pickle\n",
        "\n",
        "import time \n",
        "\n",
        "from sklearn.metrics import  average_precision_score,  roc_curve, confusion_matrix, precision_score, recall_score, auc\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device_cpu = torch.device(\"cpu\")\n",
        "device_cpu = device\n",
        "print( device )\n",
        "\n",
        "mytype = torch.float16 # to save memory (only on GPU)\n",
        "mytype = torch.float32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install rdkit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Download from a GitHub repo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Download utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget -q https://github.com/Guichaoua/komet/raw/main/komet/komet.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import komet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Download train/val/test\n",
        "Exemples of dataset : \n",
        "* LCIdb/Orphan, \n",
        "* BIOSNAP(full_data,unseen_drug, unseen_protein), \n",
        "* BindingDB. \n",
        "  \n",
        "These last datasets can be also downloaded on MolTrans Github ex: https://raw.githubusercontent.com/kexinhuang12345/MolTrans/master/dataset/DAVIS/test.csv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir data/\n",
        "!wget -q https://github.com/Guichaoua/komet/raw/main/data/LCIdb/Orphan/test.csv\n",
        "!mv test.csv data/\n",
        "!wget -q https://github.com/Guichaoua/komet/raw/main/data/LCIdb/Orphan/train.csv.zip\n",
        "!mv train.csv.zip data/\n",
        "!wget -q https://github.com/Guichaoua/komet/raw/main/data/LCIdb/Orphan/val.csv\n",
        "!mv val.csv data/\n",
        "!wget -q https://github.com/Guichaoua/komet/raw/main/data/dict_ind2fasta_all.data\n",
        "!mv dict_ind2fasta_all.data data/\n",
        "!wget -q https://github.com/Guichaoua/komet/raw/main/data/U_small.npy\n",
        "!mv U_small.npy data/\n",
        "!wget -q https://github.com/Guichaoua/komet/raw/main/data/Lambda_small.npy\n",
        "!mv Lambda_small.npy data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_dir = \"data/\"\n",
        "\n",
        "# load data\n",
        "train = komet.load_df(\"train.csv.zip\",dataset_dir)\n",
        "val = komet.load_df(\"val.csv\",dataset_dir)\n",
        "test = komet.load_df(\"test.csv\",dataset_dir)\n",
        "\n",
        "# dataframe full has all smiles and fasta sequences\n",
        "full = pd.concat([train, val, test])\n",
        "print(\"full shape\",full.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Choose parameters of approximation "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mM = 3000  #number of molecules to compute molecular features\n",
        "dM =  1000  #dimension of molecular features\n",
        "dP = 1200  # number of proteins to compute protein features (rP <=1200 because of the size of U_small.npy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Calculation of molecule features using a subsample of train molecules (molecule kernel approximated via Nystrom approximation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#### MOLECULE####\n",
        "# Index of the smiles in the dataset\n",
        "list_smiles = full[['SMILES']].drop_duplicates().values.flatten()\n",
        "nM = len(list_smiles)\n",
        "print(\"number of different smiles (mol):\",nM)\n",
        "dict_smiles2ind = {list_smiles[i]:i for i in range(nM)}\n",
        "\n",
        "# add indsmiles in train, val, test\n",
        "train['indsmiles'] = train['SMILES'].apply(lambda x:dict_smiles2ind[x] )\n",
        "val['indsmiles'] = val['SMILES'].apply(lambda x: dict_smiles2ind[x])\n",
        "test['indsmiles'] = test['SMILES'].apply(lambda x: dict_smiles2ind[x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# compute the Nystrom approximation of the mol kernel and the features of the Kronecker kernel\n",
        "nM = len(smiles)\n",
        "rM = min(mM,nM) # number of molecule to compute nystrom\n",
        "dM = min(dM,nM) # final dimension of features for molecules\n",
        "print(\"rM\",rM,\"dM\",dM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rHKuAbKb6xs",
        "outputId": "35d50350-e0d8-4b30-bc7a-e011eb096b42"
      },
      "outputs": [],
      "source": [
        "# molecule kernel_first step : computation of Morgan fingerprint for all molecules\n",
        "MorganFP = Morgan_FP(smiles)\n",
        "S = np.random.permutation(nM)[:rM]\n",
        "S = np.sort(S)\n",
        "K_S = ( MorganFP[S,:] @ MorganFP.T ) / ( 1024 - (1-MorganFP[S,:]) @ (1-MorganFP.T) )\n",
        "print(\"K_S shape\",K_S.shape)\n",
        "K_SS = K_S[:,S]\n",
        "print(\"K_SS shape\",K_SS.shape)\n",
        "# compute the approximate mol features with SVD\n",
        "V, Mu, VT = torch.svd(K_SS)\n",
        "#compute the molecule features\n",
        "epsi = 1e-8  # be careful when we divide by Mu near 0\n",
        "X = K_S.T @ V[:,:dM] @ torch.diag(1./torch.sqrt(epsi + Mu[:dM]))\n",
        "print(\"mol features shape\",X.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot the cumulative sum of the eigenvalues to choose the dimension of the features\n",
        "from matplotlib import pyplot as plt\n",
        "Mu_c = (Mu**2).cpu().numpy()\n",
        "plt.plot(np.cumsum(Mu_c )/Mu_c.sum()) # plot the cumulative sum of the eigenvalues\n",
        "# plot an horizontal line y = 0.99\n",
        "plt.plot([0,len(Mu)],[0.99,0.99], 'r--')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3S4lGo5n77C"
      },
      "source": [
        "## 4. Approximated protein features, using SVD of the Local Alignment kernel precalculated on 20605 human proteins \n",
        "\n",
        "$K_P = U \\Lambda U.T$  \n",
        "\n",
        "With file size constraints on Github, we can only load $U_{small} = U[:,:1200]$ and $\\Lambda_{small} = \\Lambda[:,:1200]$\n",
        "\n",
        "You can look in dict_ind2fasta_all.data which proteins are present."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Precomputed kernel and dict for the proteins\n",
        "dict_ind2fasta_all = pickle.load(open(\"data/dict_ind2fasta_all.data\", 'rb'))\n",
        "dict_fasta2ind_all = {fasta:ind for ind,fasta in dict_ind2fasta_all.items()}\n",
        "U = torch.from_numpy(np.load(\"data/U_small.npy\"))\n",
        "Lambda = torch.from_numpy(np.load(\"data/Lambda_small.npy\"))\n",
        "print(\"U.shape\",U.shape,\"Lambda.shape\",Lambda.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "Lambda_c = Lambda**2\n",
        "plt.plot(np.cumsum(Lambda_c )/Lambda_c.sum()) # 99% of the energy is in the first 5000 features\n",
        "# plot an horizontal line y = 0.95\n",
        "plt.plot([0,len(Lambda)],[0.95,0.95], 'r--')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# computation of feature for protein (no nystrom, just SVD)\n",
        "dP = min(U.shape[0],1200)\n",
        "Y_all = U[:,:dP] @ torch.diag(torch.sqrt(Lambda[:dP]))\n",
        "Y_all = U[:,:dP] @ torch.diag(torch.sqrt(Lambda[:dP]))\n",
        "Y_all = Y_all.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Index of the protein in the dataset\n",
        "train, fasta = add_indfasta(train)\n",
        "nP = len(fasta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "I_fasta = [dict_fasta2ind_all[fasta[i]] for i in range(len(fasta))] # index of fasta in the precomputed dict and protein kernel, in the same order as the dataset\n",
        "Y = Y_all[I_fasta,:]\n",
        "print(\"features shape\",Y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3XpBFstnvcX"
      },
      "source": [
        "### INDEX OF INTERACTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TRAIN\n",
        "I, J, y = load_datas(train)\n",
        "n = len(I)\n",
        "print(\"len(train)\",n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGIdvdGdiFgC"
      },
      "source": [
        "## Komet algorithm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### training with a choosen lambda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lamb = 0.01\n",
        "# train the model\n",
        "w,b,history_lbfgs_SVM = SVM_bfgs(X,Y,y,I,J,lamb,niter = 50)\n",
        "# compute a probability using weights (Platt scaling)\n",
        "s,t,history_lbfgs_Platt = compute_proba_Platt_Scalling(w,X,Y,y,I,J)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### validation "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load data\n",
        "val = load_df(\"val.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#### MOLECULE####\n",
        "# add index and smiles to val\n",
        "val,smiles_val =  add_indsmiles(val)\n",
        "\n",
        "# molecule kernel_first step : computation of Morgan fingerprint\n",
        "MorganFP_val= Morgan_FP(smiles_val)\n",
        "\n",
        "# compute the Nystrom approximation of the mol kernel and the features of the Kronecker kernel\n",
        "K_S_val = ( MorganFP[S,:] @ MorganFP_val.T ) / ( 1024 - (1-MorganFP[S,:]) @ (1-MorganFP_val.T) )\n",
        "print(\"K_S_val shape\",K_S_val.shape)\n",
        "\n",
        "X_val = K_S_val.T @ V[:,:dM] @ torch.diag(1./torch.sqrt(epsi + Mu[:dM]))\n",
        "print(\"mol features val shape\",X_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#### protein features for validation set\n",
        "# Index of the protein in the dataset\n",
        "val, fasta_val = add_indfasta(val)\n",
        "\n",
        "I_fasta_val = [dict_fasta2ind_all[fasta_val[i]] for i in range(len(fasta_val))] # index of fasta in the precomputed dict and protein kernel, in the same order as the dataset\n",
        "\n",
        "Y_val = Y_all[I_fasta_val,:]\n",
        "print(\"prot features val shape\",Y_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# val\n",
        "#VALIDATION \n",
        "I_val, J_val, y_val = load_datas(val)\n",
        "n_val = len(I_val)\n",
        "print(\"len(val)\",n_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# accuracy on the validation set\n",
        "m,y_pred_val, proba_pred_val = compute_proba(w,b,s,t,X_val,Y_val,I_val,J_val)\n",
        "# we compute the results\n",
        "acc1,au_Roc,au_PR,thred_optim,acc_best,cm,FP = results(y_val.cpu(),y_pred_val.cpu(),proba_pred_val.cpu())\n",
        "print(\"roc AUC = \",au_Roc)\n",
        "print(\"AUPR = \",au_PR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Choice of $\\lambda$:  Search for the best lambda by choosing the best AUPR on the validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#### Choice of the hyperparameters ####\n",
        "# we use the validation set to choose the hyperparameters\n",
        "# we use the AUPR as a criterion\n",
        "lambdas = np.logspace(-11, 2, num=14)\n",
        "accs = []\n",
        "aupr = []\n",
        "lambda_max = 0\n",
        "aupr_max = 0\n",
        "\n",
        "for lamb in lambdas:\n",
        "    print(\"lambda=\",lamb)\n",
        "\n",
        "    # train the model\n",
        "    w,b,h = SVM_bfgs(X,Y,y,I,J,lamb)\n",
        "    # compute a probability using weights (Platt scaling)\n",
        "    s,t,h = compute_proba_Platt_Scalling(w,X,Y,y,I,J)\n",
        "\n",
        "    # accuracy on the validation set\n",
        "    m,y_pred_val, proba_pred_val = compute_proba(w,b,s,t,X_val,Y_val,I_val,J_val)\n",
        "    # we compute the results\n",
        "    acc1,au_Roc,au_PR,thred_optim,acc_best,cm,FP = results(y_val.cpu(),y_pred_val.cpu(),proba_pred_val.cpu())\n",
        "\n",
        "    accs.append(acc1)\n",
        "    aupr.append(au_PR)\n",
        "\n",
        "    if au_PR > aupr_max:\n",
        "        lambda_max=lamb\n",
        "        aupr_max = au_PR\n",
        "\n",
        "print(\"-\"*30)\n",
        "print(\"lambda_max\",lambda_max)\n",
        "print(\"aupr_max for val\",aupr_max)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot accuracy in function of lambda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(lambdas,accs)\n",
        "plt.xscale('log')\n",
        "plt.title('Accuracy in function of lambda')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(lambdas,aupr)\n",
        "plt.xscale('log')\n",
        "plt.title('AUPR in function of lambda')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHRGD0YNiJ2I"
      },
      "source": [
        "## Training with lambda_max"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lamb = lambda_max\n",
        "print(\"lambda_max=\",lamb)\n",
        "# train the model\n",
        "w_bfgs,b_bfgs = SVM_bfgs(X,Y,y,I,J,lamb)\n",
        "# compute a probability using weights (Platt scaling)\n",
        "s,t = compute_proba_Platt_Scalling(w_bfgs,X,Y,y,I,J)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load data\n",
        "test = load_df(\"test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#### MOLECULE features for test set\n",
        "# Index of the smiles in the dataset\n",
        "test,smiles_test =  add_indsmiles(test)\n",
        "\n",
        "X_test = Nystrom_X(smiles_test,S,MorganFP,V,dM,Mu,epsi)\n",
        "print(\"mol features test shape\",X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#### protein features for test set\n",
        "# Index of the protein in the dataset\n",
        "test,fasta_test = add_indfasta(test)\n",
        "\n",
        "I_fasta_test = [dict_fasta2ind_all[fasta_test[i]] for i in range(len(fasta_test))] # index of fasta in the precomputed dict and protein kernel, in the same order as the dataset\n",
        "\n",
        "Y_test = Y_all[I_fasta_test,:]\n",
        "print(\"prot features test shape\",Y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# test\n",
        "I_test, J_test, y_test = load_datas(test)\n",
        "n_test = len(I_test)\n",
        "print(\"len(test)\",n_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# we compute a probability using weights (Platt scaling)\n",
        "m,y_pred, proba_pred = compute_proba(w_bfgs,b_bfgs,s,t,X_test,Y_test,I_test,J_test)\n",
        "# we compute the results\n",
        "acc1,au_Roc,au_PR,thred_optim,acc_best,cm,FP = results(y_test.cpu(),y_pred.cpu(),proba_pred.cpu())\n",
        "print(f\"roc AUC = {au_Roc:.4f}\")\n",
        "print(f\"AUPR = {au_PR:.4f}\")\n",
        "print(f\"accuracy (threshold 0.5)= {acc1:.4f}\")\n",
        "print(f\"best threshold = {thred_optim:.4f}\")\n",
        "print(f\"accuracy (best threshold)= {acc_best:.4f}\")\n",
        "print(f\"false positive (best threshold)= {FP:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot confusion matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "labels = [-1., 1.]\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                              display_labels=labels)\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot distribution (density) of p when y_test=1\n",
        "plt.hist(proba_pred.cpu().numpy()[y_test.cpu().numpy()==1],bins=10,alpha=0.8,color='green',label='y_test=1');\n",
        "plt.hist(proba_pred.cpu().numpy()[y_test.cpu()==-1],bins=10,alpha=0.5,color='red',label='y_test=-1');\n",
        "plt.legend()\n",
        "plt.title('Distribution of predicted probability')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
